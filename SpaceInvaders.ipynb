{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpaceInvaders.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSsg7THm-Ybh",
        "colab_type": "text"
      },
      "source": [
        "My models consistently reach 1.5k or 2k reward in the first episode, and then fall down to sub 1000 reward.\n",
        "I followed the basic cnn structure from the tutorials I saw.\n",
        "Filters gradually increasing, pooling layers between convolutional layers. I didn't use kernal_size of 5 because I figured I was already down_sampling my image.\n",
        "One major problem was that since I was running this on the colab GPU, I could not run the gym render() function. I had no clue what my model was actually doing in game, and I could only base everything off of reward.\n",
        "\n",
        "I also couldn't really check if I was feeding data in properly to my neural network, there's a chance I messed up feeding in the reshaped data.\n",
        "\n",
        "If I had more time, I would mess around with activation of the conv2d layers, and I would try more optimizers.\n",
        "\n",
        "I would also try changing epsilon value of the model, in retropect, an episolon of 0.1 seems pretty high in a game where one alien bullet will delete your entire run. \n",
        "\n",
        "More importantly, I would want to try making more drastic changes to my cnn structure, maybe more conv2d layers before pooling layers, maybe more dense layers. \n",
        "\n",
        "Most of the code is taken from the reinforcement learning assignment, I have taken out most of those comments so it will be easier to find my comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-C6gJuJ_xD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, MaxPooling2D, MaxPooling1D, Conv2D, Conv1D\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, Adagrad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQOIMeVHBUsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "epsilon = .1 \n",
        "num_actions = 6  \n",
        "max_memory = 500\n",
        "hidden_size = 100\n",
        "batch_size = 50\n",
        "#I don't use the grid_size variable in the end\n",
        "grid_size = 105*80\n",
        "print_freq = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eL36sQABWCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ExperienceReplay(object):\n",
        "    def __init__(self, max_memory=100, discount=.9):\n",
        "        self.max_memory = max_memory\n",
        "        self.memory = list()\n",
        "        self.discount = discount\n",
        "\n",
        "    def remember(self, states, game_over):\n",
        "        '''\n",
        "        Input:\n",
        "            states: [starting_observation, action_taken, reward_received, new_observation]\n",
        "            game_over: boolean\n",
        "        Add the states and game over to the internal memory array. If the array is longer than\n",
        "        self.max_memory, drop the oldest memory\n",
        "        '''\n",
        "        self.memory.append([states, game_over])\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            del self.memory[0]\n",
        "\n",
        "    def get_batch(self, model, batch_size=10):\n",
        "        '''\n",
        "        Randomly chooses batch_size memories, possibly repeating.\n",
        "        For each of these memories, updates the models current best guesses about the value of taking a\n",
        "            certain action from the starting state, based on the reward received and the model's current\n",
        "            estimate of how valuable the new state is.\n",
        "        '''\n",
        "        len_memory = len(self.memory)\n",
        "        num_actions = model.output_shape[-1] # the number of possible actions\n",
        "        # I don't use env_dim in the ned\n",
        "        # env_dim = self.memory[0][0][0].shape[1] # the number of pixels in the image\n",
        "        # env_dim = 105*80\n",
        "        input_size = min(len_memory, batch_size)\n",
        "\n",
        "        #Instead of inputs = np.zeros((input_size, env_dim)), I did this, I don't know how to use variables in this case\n",
        "        inputs = np.zeros((input_size, 105, 80))\n",
        "        targets = np.zeros((input_size, num_actions))\n",
        "        for i, idx in enumerate(np.random.randint(0, len_memory, size=input_size)):\n",
        "            starting_observation, action_taken, reward_received, new_observation = self.memory[idx][0]\n",
        "            game_over = self.memory[idx][1]\n",
        "\n",
        "            inputs[i] = starting_observation\n",
        "            \n",
        "            targets[i] = model.predict(starting_observation.reshape(-1,105,80,1))[0]\n",
        "            # targets[i] = model.predict(starting_observation)[0]\n",
        "                               \n",
        "            if game_over: \n",
        "                targets[i, action_taken] = reward_received\n",
        "            else:\n",
        "                Q_sa = np.max(model.predict(new_observation.reshape(-1,105,80,1))[0])\n",
        "                # Q_sa = np.max(model.predict(new_observation)[0])\n",
        "                targets[i, action_taken] = reward_received + self.discount * Q_sa\n",
        "        return inputs, targets\n",
        "        \n",
        "def build_model():\n",
        "    '''\n",
        "     Returns three initialized objects: the model, the environment, and the replay.\n",
        "    '''\n",
        "    model = Sequential()\n",
        "\n",
        "    # This time, I'm making the model use very few filters and very few layers\n",
        "    model.add(Conv2D(input_shape=(105, 80, 1),\n",
        "                 filters=8, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Conv2D(filters=16, kernel_size=1, strides=1, padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=32, activation='relu'))\n",
        "    model.add(Dense(units=num_actions, activation='relu'))\n",
        "    model.compile(Adam(), \"mse\")\n",
        "    \n",
        "    # didn't work\n",
        "    # model.add(Conv2D(input_shape=(105, 80, 1),\n",
        "    #              filters=8, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(Conv2D(filters=16, kernel_size=1, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # model.add(Dropout(0.10))\n",
        "    # model.add(Conv2D(filters=32, kernel_size=1, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(Conv2D(filters=64, kernel_size=1, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # model.add(Flatten())\n",
        "    # model.add(Dense(units=32, activation='relu'))\n",
        "    # model.add(Dense(units=num_actions, activation='relu'))\n",
        "    # model.compile(Adam(), \"mse\")\n",
        "\n",
        "    # Tried a different optimizer, also tried only using one conv2d layer between pooling layers\n",
        "    # model.add(Conv2D(input_shape=(105, 80, 1),\n",
        "    #              filters=16, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # # model.add(Dropout(0.10))\n",
        "    # model.add(Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # model.add(Conv2D(filters=64, kernel_size=1, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # model.add(Flatten())\n",
        "    # model.add(Dense(units=64, activation='relu'))\n",
        "    # model.add(Dense(units=num_actions, activation='relu'))\n",
        "    # model.compile(RMSprop(), \"mse\")\n",
        "\n",
        "    #Tried increasing convolutional layer filters and taking away dense layer units\n",
        "    # model.add(Conv2D(input_shape=(105, 80, 1),\n",
        "    #              filters=32, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # model.add(Conv2D(filters=64, kernel_size=1, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # model.add(Flatten())\n",
        "    # model.add(Dense(units=32, activation='relu'))\n",
        "    # model.add(Dense(units=num_actions, activation='relu'))\n",
        "    # model.compile(Adam(), \"mse\")\n",
        "\n",
        "    # Tried making a bunch of pooling layers and adding a ton of filters\n",
        "    # model.add(Conv2D(input_shape=(105, 80, 1),\n",
        "    #              filters=32, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # model.add(Dropout(0.20))\n",
        "    # model.add(Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # model.add(Dropout(0.20))\n",
        "    # model.add(Conv2D(filters=128, kernel_size=1, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # model.add(Dropout(0.20))\n",
        "    # model.add(Conv2D(filters=256, kernel_size=1, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # model.add(Dropout(0.20))\n",
        "    # model.add(Flatten())\n",
        "    # model.add(Dense(units=32, activation='relu'))\n",
        "    # model.add(Dense(units=num_actions, activation='relu'))\n",
        "    # model.compile(Adam(), \"mse\")\n",
        "\n",
        "    # I tried making the model more simple\n",
        "    # model.add(Conv2D(input_shape=(105, 80, 1),\n",
        "    #              filters=16, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # model.add(Dropout(0.20))\n",
        "    # model.add(Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # model.add(Dropout(0.20))\n",
        "    # model.add(Flatten())\n",
        "    # model.add(Dense(units=32, activation='relu'))\n",
        "    # model.add(Dense(units=num_actions, activation='relu'))\n",
        "    # model.compile(Adam(), \"mse\")\n",
        "\n",
        "    # Tried taking out dropout layers, \n",
        "    # model.add(Conv2D(input_shape=(105, 80, 1),\n",
        "    #              filters=16, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "    # I think kernel_size should be 1 here\n",
        "    # model.add(Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # model.add(Flatten())\n",
        "    # model.add(Dense(units=32, activation='relu'))\n",
        "    # model.add(Dense(units=num_actions, activation='relu'))\n",
        "    # model.compile(Adam(), \"mse\")\n",
        "\n",
        "    # Tried a different optimizer\n",
        "    # model.add(Conv2D(input_shape=(105, 80, 1),\n",
        "    #              filters=16, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # model.add(Conv2D(filters=32, kernel_size=1, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # model.add(Flatten())\n",
        "    # model.add(Dense(units=32, activation='relu'))\n",
        "    # model.add(Dense(units=num_actions, activation='relu'))\n",
        "    # model.compile(Adagrad(), \"mse\")\n",
        "    \n",
        "    # Define environment/game\n",
        "    env = gym.make('SpaceInvaders-v0')\n",
        "\n",
        "    exp_replay = ExperienceReplay(max_memory=max_memory)\n",
        "    \n",
        "    return model, env, exp_replay\n",
        "\n",
        "def take_step(exp_replay, model, starting_observation):\n",
        "    if np.random.rand() <= epsilon:\n",
        "        action = np.random.randint(0, num_actions, size=1)\n",
        "    else:\n",
        "        q = model.predict(starting_observation.reshape(-1,105,80,1))\n",
        "        #q = model.predict(starting_observation)\n",
        "        action = np.argmax(q[0])\n",
        "\n",
        "    new_observation, reward, game_over, info = env.step(action)\n",
        "    new_observation = preprocess(new_observation)\n",
        "    exp_replay.remember([starting_observation, action, reward, new_observation], game_over)\n",
        "\n",
        "    return new_observation, reward, game_over\n",
        "    \n",
        "\n",
        "def train_model(model, env, exp_replay, num_episodes, pretrain_episodes=100):\n",
        "    '''\n",
        "    Inputs:\n",
        "        model, env, and exp_replay objects as returned by build_model\n",
        "        num_episodes: integer, the number of episodes that should be rolled out for training\n",
        "    '''\n",
        "    for episode in range(pretrain_episodes):\n",
        "        game_over = False\n",
        "        # get initial input\n",
        "        starting_observation = env.reset()\n",
        "        starting_observation = preprocess(starting_observation)\n",
        "        while not game_over:\n",
        "            starting_observation, reward, game_over = \\\n",
        "                take_step(exp_replay, model, starting_observation)\n",
        "    last_total_reward = 0\n",
        "    total_reward = 0\n",
        "    for episode in range(1, num_episodes+1):\n",
        "        loss = 0.\n",
        "        game_over = False\n",
        "        # get initial input\n",
        "        starting_observation = env.reset()\n",
        "        starting_observation = preprocess(starting_observation)\n",
        "        while not game_over:\n",
        "            starting_observation, reward, game_over = \\\n",
        "                take_step(exp_replay, model, starting_observation)\n",
        "            total_reward += reward\n",
        "            \n",
        "        inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
        "\n",
        "        loss += model.train_on_batch(inputs.reshape(-1,105,80,1), targets)          \n",
        "        #loss += model.train_on_batch(inputs, targets)  \n",
        "\n",
        "        # Print update from this episode\n",
        "        if episode % print_freq == 0:\n",
        "            print(\"Episodes {:04d}-{:04d}/{:04d} | Loss {:.4f} | Total Reward {} | Change in Reward {}\".format(\n",
        "                episode - print_freq + 1, episode, num_episodes, loss, total_reward, total_reward - last_total_reward))\n",
        "            last_total_reward = total_reward\n",
        "            total_reward = 0\n",
        "\n",
        "#I found this code from here https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26\n",
        "def to_grayscale(img):\n",
        "    return np.mean(img, axis=2).astype(np.uint8)\n",
        "def downsample(img):\n",
        "    return img[::2, ::2]\n",
        "def preprocess(img):\n",
        "    return to_grayscale(downsample(img))\n",
        "\n",
        "#Doesn't work with colab\n",
        "def play_game(model, env):\n",
        "    last_observation = env.reset()\n",
        "    last_observation = preprocess(last_observation)\n",
        "    game_over = False\n",
        "    total_reward = 0\n",
        "    while not game_over:\n",
        "        q = model.predict(last_observation)\n",
        "        action = np.argmax(q[0])\n",
        "        last_observation, reward, game_over, info = env.step(action)\n",
        "        last_observation = preprocess(last_observation)\n",
        "        total_reward += reward\n",
        "        env.render()\n",
        "    print(total_reward)\n",
        "    env.close()\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJM7t9xTBYJW",
        "colab_type": "code",
        "outputId": "fcb26518-33b1-43ee-bea9-9a02dafbcfcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model, env, exp_replay = build_model()\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 105, 80, 8)        80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 52, 40, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 52, 40, 16)        144       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 26, 20, 16)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 8320)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                266272    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 266,694\n",
            "Trainable params: 266,694\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TTGvRvXTptg",
        "colab_type": "code",
        "outputId": "a3f593b8-3044-4670-b07a-498781166047",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "train_model(model, env, exp_replay, num_episodes=500)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episodes 0001-0010/0500 | Loss 1.4195 | Total Reward 1960.0 | Change in Reward 1960.0\n",
            "Episodes 0011-0020/0500 | Loss 1.5565 | Total Reward 1615.0 | Change in Reward -345.0\n",
            "Episodes 0021-0030/0500 | Loss 0.0000 | Total Reward 1130.0 | Change in Reward -485.0\n",
            "Episodes 0031-0040/0500 | Loss 3.0466 | Total Reward 1335.0 | Change in Reward 205.0\n",
            "Episodes 0041-0050/0500 | Loss 0.0004 | Total Reward 875.0 | Change in Reward -460.0\n",
            "Episodes 0051-0060/0500 | Loss 0.0000 | Total Reward 630.0 | Change in Reward -245.0\n",
            "Episodes 0061-0070/0500 | Loss 0.3333 | Total Reward 740.0 | Change in Reward 110.0\n",
            "Episodes 0071-0080/0500 | Loss 1.7500 | Total Reward 575.0 | Change in Reward -165.0\n",
            "Episodes 0081-0090/0500 | Loss 2.0833 | Total Reward 690.0 | Change in Reward 115.0\n",
            "Episodes 0091-0100/0500 | Loss 0.8181 | Total Reward 1200.0 | Change in Reward 510.0\n",
            "Episodes 0101-0110/0500 | Loss 1.3333 | Total Reward 450.0 | Change in Reward -750.0\n",
            "Episodes 0111-0120/0500 | Loss 0.0063 | Total Reward 665.0 | Change in Reward 215.0\n",
            "Episodes 0121-0130/0500 | Loss 0.0000 | Total Reward 810.0 | Change in Reward 145.0\n",
            "Episodes 0131-0140/0500 | Loss 0.0000 | Total Reward 515.0 | Change in Reward -295.0\n",
            "Episodes 0141-0150/0500 | Loss 0.0000 | Total Reward 815.0 | Change in Reward 300.0\n",
            "Episodes 0151-0160/0500 | Loss 1.3333 | Total Reward 690.0 | Change in Reward -125.0\n",
            "Episodes 0161-0170/0500 | Loss 0.0000 | Total Reward 975.0 | Change in Reward 285.0\n",
            "Episodes 0171-0180/0500 | Loss 0.0000 | Total Reward 915.0 | Change in Reward -60.0\n",
            "Episodes 0181-0190/0500 | Loss 0.0000 | Total Reward 805.0 | Change in Reward -110.0\n",
            "Episodes 0191-0200/0500 | Loss 0.3569 | Total Reward 830.0 | Change in Reward 25.0\n",
            "Episodes 0201-0210/0500 | Loss 0.0000 | Total Reward 595.0 | Change in Reward -235.0\n",
            "Episodes 0211-0220/0500 | Loss 0.0000 | Total Reward 875.0 | Change in Reward 280.0\n",
            "Episodes 0221-0230/0500 | Loss 0.3333 | Total Reward 710.0 | Change in Reward -165.0\n",
            "Episodes 0231-0240/0500 | Loss 0.0000 | Total Reward 685.0 | Change in Reward -25.0\n",
            "Episodes 0241-0250/0500 | Loss 0.0000 | Total Reward 750.0 | Change in Reward 65.0\n",
            "Episodes 0251-0260/0500 | Loss 0.0833 | Total Reward 810.0 | Change in Reward 60.0\n",
            "Episodes 0261-0270/0500 | Loss 0.0000 | Total Reward 730.0 | Change in Reward -80.0\n",
            "Episodes 0271-0280/0500 | Loss 0.0000 | Total Reward 590.0 | Change in Reward -140.0\n",
            "Episodes 0281-0290/0500 | Loss 0.0000 | Total Reward 455.0 | Change in Reward -135.0\n",
            "Episodes 0291-0300/0500 | Loss 0.0000 | Total Reward 810.0 | Change in Reward 355.0\n",
            "Episodes 0301-0310/0500 | Loss 0.0000 | Total Reward 1185.0 | Change in Reward 375.0\n",
            "Episodes 0311-0320/0500 | Loss 0.0000 | Total Reward 775.0 | Change in Reward -410.0\n",
            "Episodes 0321-0330/0500 | Loss 0.7500 | Total Reward 705.0 | Change in Reward -70.0\n",
            "Episodes 0331-0340/0500 | Loss 0.0000 | Total Reward 340.0 | Change in Reward -365.0\n",
            "Episodes 0341-0350/0500 | Loss 0.0000 | Total Reward 755.0 | Change in Reward 415.0\n",
            "Episodes 0351-0360/0500 | Loss 0.0000 | Total Reward 500.0 | Change in Reward -255.0\n",
            "Episodes 0361-0370/0500 | Loss 0.0000 | Total Reward 490.0 | Change in Reward -10.0\n",
            "Episodes 0371-0380/0500 | Loss 0.3333 | Total Reward 715.0 | Change in Reward 225.0\n",
            "Episodes 0381-0390/0500 | Loss 0.0000 | Total Reward 955.0 | Change in Reward 240.0\n",
            "Episodes 0391-0400/0500 | Loss 0.0000 | Total Reward 960.0 | Change in Reward 5.0\n",
            "Episodes 0401-0410/0500 | Loss 0.7500 | Total Reward 860.0 | Change in Reward -100.0\n",
            "Episodes 0411-0420/0500 | Loss 0.0000 | Total Reward 670.0 | Change in Reward -190.0\n",
            "Episodes 0421-0430/0500 | Loss 0.0000 | Total Reward 725.0 | Change in Reward 55.0\n",
            "Episodes 0431-0440/0500 | Loss 0.8333 | Total Reward 540.0 | Change in Reward -185.0\n",
            "Episodes 0441-0450/0500 | Loss 0.7500 | Total Reward 890.0 | Change in Reward 350.0\n",
            "Episodes 0451-0460/0500 | Loss 0.0000 | Total Reward 750.0 | Change in Reward -140.0\n",
            "Episodes 0461-0470/0500 | Loss 0.0000 | Total Reward 805.0 | Change in Reward 55.0\n",
            "Episodes 0471-0480/0500 | Loss 1.3333 | Total Reward 565.0 | Change in Reward -240.0\n",
            "Episodes 0481-0490/0500 | Loss 0.0833 | Total Reward 920.0 | Change in Reward 355.0\n",
            "Episodes 0491-0500/0500 | Loss 0.0000 | Total Reward 670.0 | Change in Reward -250.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me03wzFlK71L",
        "colab_type": "code",
        "outputId": "c9dc2836-e2f5-4588-debd-2b39aef5e062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "#Doesn't work because env.render() doesn't work\n",
        "play_game(model, env)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-84f8a93899bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzrLfk8ZltdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This is code from above, but it works with dense layers\n",
        "#The input is't reshaped\n",
        "#I kept the original comments here\n",
        "\n",
        "class ExperienceReplay(object):\n",
        "    def __init__(self, max_memory=100, discount=.9):\n",
        "        self.max_memory = max_memory\n",
        "        self.memory = list()\n",
        "        self.discount = discount\n",
        "\n",
        "    def remember(self, states, game_over):\n",
        "        '''\n",
        "        Input:\n",
        "            states: [starting_observation, action_taken, reward_received, new_observation]\n",
        "            game_over: boolean\n",
        "        Add the states and game over to the internal memory array. If the array is longer than\n",
        "        self.max_memory, drop the oldest memory\n",
        "        '''\n",
        "        self.memory.append([states, game_over])\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            del self.memory[0]\n",
        "\n",
        "    def get_batch(self, model, batch_size=10):\n",
        "        '''\n",
        "        Randomly chooses batch_size memories, possibly repeating.\n",
        "        For each of these memories, updates the models current best guesses about the value of taking a\n",
        "            certain action from the starting state, based on the reward received and the model's current\n",
        "            estimate of how valuable the new state is.\n",
        "        '''\n",
        "        len_memory = len(self.memory)\n",
        "        num_actions = model.output_shape[-1] # the number of possible actions\n",
        "        env_dim = self.memory[0][0][0].shape[1] # the number of pixels in the image\n",
        "        input_size = min(len_memory, batch_size)\n",
        "        inputs = np.zeros((input_size, env_dim))\n",
        "        targets = np.zeros((input_size, num_actions))\n",
        "        for i, idx in enumerate(np.random.randint(0, len_memory, size=input_size)):\n",
        "            starting_observation, action_taken, reward_received, new_observation = self.memory[idx][0]\n",
        "            game_over = self.memory[idx][1]\n",
        "\n",
        "            # Set the input to the state that was observed in the game before an action was taken\n",
        "            inputs[i] = starting_observation[0]\n",
        "            \n",
        "            # Start with the model's current best guesses about the value of taking each action from this state\n",
        "            targets[i] = model.predict(starting_observation)[0]\n",
        "            \n",
        "            # Now we need to update the value of the action that was taken                      \n",
        "            if game_over: \n",
        "                # if the game is over, give the actual reward received\n",
        "                targets[i, action_taken] = reward_received\n",
        "            else:\n",
        "                # if the game is not over, give the reward received (always zero in this particular game)\n",
        "                # plus the maximum reward predicted for state we got to by taking this action (with a discount)\n",
        "                Q_sa = np.max(model.predict(new_observation)[0])\n",
        "                targets[i, action_taken] = reward_received + self.discount * Q_sa\n",
        "        return inputs, targets\n",
        "        \n",
        "def build_model():\n",
        "    '''\n",
        "     Returns three initialized objects: the model, the environment, and the replay.\n",
        "    '''\n",
        "    model = Sequential()\n",
        "    model.add(Dense(hidden_size, input_shape=(80,), activation='relu'))\n",
        "    model.add(Dense(hidden_size, activation='relu'))\n",
        "    model.add(Dense(hidden_size, activation='relu'))\n",
        "    model.add(Dense(num_actions))\n",
        "    \n",
        "    # model.add(Conv2D(input_shape=(105, 80, 1),\n",
        "    #              filters=32, kernel_size=5, strides=1, padding='same', activation='tanh'))\n",
        "    # model.add(Conv2D(filters=32, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    # model.add(Dropout(0.15))\n",
        "    # model.add(Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='tanh'))\n",
        "    # model.add(Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "    # model.add(Flatten())\n",
        "    # model.add(Dense(units=32, activation='relu'))\n",
        "    # model.add(Dense(units=num_actions, activation='relu'))\n",
        "    # model.compile(RMSprop(), \"mse\")\n",
        "    model.compile(Adam(), \"mse\")\n",
        "\n",
        "    # Define environment/game\n",
        "    env = gym.make('SpaceInvaders-v0')\n",
        "\n",
        "    # Initialize experience replay object\n",
        "    exp_replay = ExperienceReplay(max_memory=max_memory)\n",
        "    \n",
        "    return model, env, exp_replay\n",
        "\n",
        "def take_step(exp_replay, model, starting_observation):\n",
        "    # get next action\n",
        "    if np.random.rand() <= epsilon:\n",
        "        # epsilon of the time, we just choose randomly\n",
        "        action = np.random.randint(0, num_actions, size=1)\n",
        "    else:\n",
        "        # find which action the model currently thinks is best from this state\n",
        "        q = model.predict(starting_observation)\n",
        "        action = np.argmax(q[0])\n",
        "\n",
        "    # apply action, get rewards and new state\n",
        "    new_observation, reward, game_over, info = env.step(action)\n",
        "    new_observation = preprocess(new_observation)\n",
        "    # store experience\n",
        "    exp_replay.remember([starting_observation, action, reward, new_observation], game_over)\n",
        "\n",
        "    return new_observation, reward, game_over\n",
        "    \n",
        "\n",
        "def train_model(model, env, exp_replay, num_episodes, pretrain_episodes=10):\n",
        "    '''\n",
        "    Inputs:\n",
        "        model, env, and exp_replay objects as returned by build_model\n",
        "        num_episodes: integer, the number of episodes that should be rolled out for training\n",
        "    '''\n",
        "    for episode in range(pretrain_episodes):\n",
        "        game_over = False\n",
        "        # get initial input\n",
        "        starting_observation = env.reset()\n",
        "        starting_observation = preprocess(starting_observation)\n",
        "        while not game_over:\n",
        "            starting_observation, reward, game_over = \\\n",
        "                take_step(exp_replay, model, starting_observation)\n",
        "    last_total_reward = 0\n",
        "    total_reward = 0\n",
        "    for episode in range(1, num_episodes+1):\n",
        "        loss = 0.\n",
        "        game_over = False\n",
        "        # get initial input\n",
        "        starting_observation = env.reset()\n",
        "        starting_observation = preprocess(starting_observation)\n",
        "        while not game_over:\n",
        "            starting_observation, reward, game_over = \\\n",
        "                take_step(exp_replay, model, starting_observation)\n",
        "            total_reward += reward\n",
        "            \n",
        "        # get data updated based on the stored experiences\n",
        "        inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
        "\n",
        "        # train model on the updated data        \n",
        "        loss += model.train_on_batch(inputs, targets)  \n",
        "\n",
        "        # Print update from this episode\n",
        "        if episode % print_freq == 0:\n",
        "            print(\"Episodes {:04d}-{:04d}/{:04d} | Loss {:.4f} | Total Reward {} | Change in Reward {}\".format(\n",
        "                episode - print_freq + 1, episode, num_episodes, loss, total_reward, total_reward - last_total_reward))\n",
        "            last_total_reward = total_reward\n",
        "            total_reward = 0\n",
        "\n",
        "#I found this code from here https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26\n",
        "def to_grayscale(img):\n",
        "    return np.mean(img, axis=2).astype(np.uint8)\n",
        "\n",
        "def downsample(img):\n",
        "    return img[::2, ::2]\n",
        "\n",
        "def preprocess(img):\n",
        "    return to_grayscale(downsample(img))\n",
        "def play_game(model, env):\n",
        "    #I'm still preprocessing, which is sketch\n",
        "    last_observation = env.reset()\n",
        "    last_observation = preprocess(last_observation)\n",
        "    game_over = False\n",
        "    total_reward = 0\n",
        "    while not game_over:\n",
        "        q = model.predict(last_observation)\n",
        "        action = np.argmax(q[0])\n",
        "        last_observation, reward, game_over, info = env.step(action)\n",
        "        last_observation = preprocess(last_observation)\n",
        "        total_reward += reward\n",
        "        env.render()\n",
        "    print(total_reward)\n",
        "    env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4YV1opBmIY2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "73c30455-1dc5-48fb-91c0-2892ce5a4dca"
      },
      "source": [
        "model, env, exp_replay = build_model()\n",
        "model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (None, 100)               8100      \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 6)                 606       \n",
            "=================================================================\n",
            "Total params: 28,906\n",
            "Trainable params: 28,906\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6henwBrhArsH",
        "colab_type": "text"
      },
      "source": [
        "This model somehow does way better than the cnn, although I think it still does pretty badly given that it's basically getting a score of 200 before dying. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDScSjIgmL9V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b8742b47-8459-45ba-fbb9-e83b632bd0c6"
      },
      "source": [
        "train_model(model, env, exp_replay, num_episodes=100)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episodes 0001-0010/0100 | Loss 0.0000 | Total Reward 1225.0 | Change in Reward 1225.0\n",
            "Episodes 0011-0020/0100 | Loss 0.0000 | Total Reward 3000.0 | Change in Reward 1775.0\n",
            "Episodes 0021-0030/0100 | Loss 2.0829 | Total Reward 2385.0 | Change in Reward -615.0\n",
            "Episodes 0031-0040/0100 | Loss 0.7498 | Total Reward 3190.0 | Change in Reward 805.0\n",
            "Episodes 0041-0050/0100 | Loss 0.0000 | Total Reward 2475.0 | Change in Reward -715.0\n",
            "Episodes 0051-0060/0100 | Loss 1.3329 | Total Reward 3060.0 | Change in Reward 585.0\n",
            "Episodes 0061-0070/0100 | Loss 0.3331 | Total Reward 2820.0 | Change in Reward -240.0\n",
            "Episodes 0071-0080/0100 | Loss 0.0832 | Total Reward 2425.0 | Change in Reward -395.0\n",
            "Episodes 0081-0090/0100 | Loss 0.0000 | Total Reward 2320.0 | Change in Reward -105.0\n",
            "Episodes 0091-0100/0100 | Loss 0.3330 | Total Reward 2615.0 | Change in Reward 295.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}